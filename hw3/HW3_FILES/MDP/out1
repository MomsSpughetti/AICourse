@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@ The board and rewards @@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
| -0.04 | -0.04 | -0.04 | +1    |
| -0.04 | WALL  | -0.04 | -1    |
| -0.04 | -0.04 | -0.04 | -0.04 |

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@ Value iteration @@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Initial utility:
| 0.0   | 0.0   | 0.0   | 0.0   |
| 0.0   | WALL  | 0.0   | 0.0   |
| 0.0   | 0.0   | 0.0   | 0.0   |


Final utility:
| 0.424 | 0.579 | 0.734 | 1.0   |
| 0.311 | WALL  | 0.332 | -1.0  |
| 0.198 | 0.116 | 0.176 | -0.07 |


Final policy:
| RIGHT | RIGHT | RIGHT | +1    |
| UP    | WALL  | UP    | -1    |
| UP    | LEFT  | UP    | DOWN  |

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@ Policy iteration @@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Policy evaluation:
| 0.424 | 0.579 | 0.734 | 1.0   |
| 0.311 | WALL  | 0.332 | -1.0  |
| 0.198 | 0.116 | 0.176 | -0.06 |


Initial policy:
| UP    | UP    | UP    | +1    |
| UP    | WALL  | UP    | -1    |
| UP    | UP    | UP    | UP    |


Final policy:
| RIGHT | RIGHT | RIGHT | +1    |
| UP    | WALL  | UP    | -1    |
| UP    | LEFT  | UP    | DOWN  |

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@ Get all policies @@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
All policies for U with Epsilon:  0.001
U: 
| 0.749 | 0.819 | 0.876 | 1.0   |
| 0.692 | WALL  | 0.564 | -1.0  |
| 0.623 | 0.566 | 0.518 | 0.252 |

All policies: 
| R     | R     | R     | +1    |
| U     | WALL  | L     | -1    |
| U     | L     | L     | D     |

Number of policies:  1


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@ Get policy for different rewards @@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
R(s) <  -1.46
| R     | R     | R     | +1    |
| U     | WALL  | R     | -1    |
| R     | R     | R     | U     |

-1.46  <= R(s) <  -1.3
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| R     | R     | R     | U     |

-1.3  <= R(s) <  -0.61
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| R     | R     | U     | U     |

-0.61  <= R(s) <  -0.6
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| UR    | R     | U     | U     |

-0.6  <= R(s) <  -0.37
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| U     | R     | U     | U     |

-0.37  <= R(s) <  -0.05
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| U     | R     | U     | L     |

-0.05  <= R(s) <  -0.04
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| U     | L     | U     | L     |

-0.04  <= R(s) <  0.01
| R     | R     | R     | +1    |
| U     | WALL  | U     | -1    |
| U     | L     | U     | D     |

0.01  <= R(s) <  0.09
| R     | R     | R     | +1    |
| U     | WALL  | L     | -1    |
| U     | L     | U     | D     |

0.09  <= R(s) <  0.1
| R     | R     | R     | +1    |
| U     | WALL  | L     | -1    |
| U     | L     | UL    | D     |

0.1  <= R(s) <  0.11
| UDRL  | UDRL  | UDRL  | +1    |
| UDRL  | WALL  | L     | -1    |
| UDRL  | UDRL  | UDRL  | D     |

0.11 <= R(s)
| UDRL  | UDRL  | L     | +1    |
| UDRL  | WALL  | L     | -1    |
| UDRL  | UDRL  | UDRL  | D     |

The rewards that change the policy:
[-1.46, -1.3, -0.61, -0.6, -0.37, -0.05, -0.04, 0.01, 0.09, 0.1, 0.11]
Done!
